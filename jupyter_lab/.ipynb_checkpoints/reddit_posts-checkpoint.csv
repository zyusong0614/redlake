subreddit,title,selftext,upvote_ratio,ups,downs,score
Python,Sunday Daily Thread: What's everyone working on this week?,"# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è

Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!

## How it Works:

1. **Show &amp; Tell**: Share your current projects, completed works, or future ideas.
2. **Discuss**: Get feedback, find collaborators, or just chat about your project.
3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.

## Guidelines:

* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.
* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.

## Example Shares:

1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!
2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.
3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!

Let's build and grow together! Share your journey and learn from others. Happy coding! üåü",0.57,1,0,1
Python,Saturday Daily Thread: Resource Request and Sharing! Daily Thread,"# Weekly Thread: Resource Request and Sharing üìö

Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!

## How it Works:

1. **Request**: Can't find a resource on a particular topic? Ask here!
2. **Share**: Found something useful? Share it with the community.
3. **Review**: Give or get opinions on Python resources you've used.

## Guidelines:

* Please include the type of resource (e.g., book, video, article) and the topic.
* Always be respectful when reviewing someone else's shared resource.

## Example Shares:

1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms.
2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures.
3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators.

## Example Requests:

1. **Looking for**: Video tutorials on web scraping with Python.
2. **Need**: Book recommendations for Python machine learning.

Share the knowledge, enrich the community. Happy learning! üåü",0.67,1,0,1
Python,"I made a game that is teaching you Python! :) After more than three years, I finally released it!","It's called The Farmer Was Replaced

Program and optimize a drone to automate a farm and watch it do the work for you. Collect resources to unlock better technology and become the most efficient farmer in the world. Improve your problem solving and coding skills.

Unlike most programming games the game isn't divided into distinct levels that you have to complete but features a continuous progression.

Farming earns you resources which can be spent to unlock new technology.

Programming is done in a simple language similar to Python. The beginning of the game is designed to teach you all the basic programming concepts you will need by introducing them one at a time.

While it introduces everything that is relevant, it won't hold your hand when it comes to solving the various tasks in the game. You will have to figure those out for yourself, and that can be very challenging if you have never programmed before.

If you are an experienced programmer, you should be able to get through the early game very quickly and move on to the more complex tasks of the later game, which should still provide interesting challenges.

Although the programming language isn't exactly Python, it's similar enough that Python IntelliSense works well with it. All code is stored in .py files and can optionally be edited using external code editors like VS Code. When the ""File Watcher"" setting is enabled, the game automatically detects external changes.

You can find it here:¬†[https://store.steampowered.com/app/2060160/The\_Farmer\_Was\_Replaced/](https://store.steampowered.com/app/2060160?utm_source=RD)",0.95,243,0,243
Python,Best practices for using Python &amp; uv inside Docker,"Getting¬†`uv`¬†right inside¬†Docker¬†is a bit tricky and even their¬†[official recommendations](https://docs.astral.sh/uv/guides/integration/docker/)¬†are not optimal.

It is better to use a [two-step build process](https://ashishb.net/programming/using-python-uv-inside-docker/) to eliminate¬†`uv`¬†from the final image size.

A two-step build process not only saves disk space but also reduces attack surface against [security vulerabilities](https://astral.sh/blog/uv-security-advisory-cve-2025-54368)",0.88,149,0,149
Python,I made a Better Notepad alternative using PySide6,"# What My Project Does

ZenNotes is a minimalistic Notepad app with a sleek design inspired by the Fluent Design. It offers the familiar look of the Windows Notepad while having much more powerful features like Translate, TTS, etc.

# Target Audience

Anyone who uses Windows Notepad, or noepads in general

# Comparison¬†

The target competition is Windows Notepad. ZenNotes is like an ""extension"" of Windows Notepad, with similar looks but much more features, like TTS, Translate, etc.

# GitHub

[https://github.com/rohankishore/ZenNotes](https://github.com/rohankishore/ZenNotes)",0.77,24,0,24
Python,"Announcing html-to-markdown v2: Rust rewrite, full CommonMark 1.2 compliance, and hOCR support","Hi Pythonistas,

I'm glad to announce the v2 release of [html-to-markdown](https://github.com/Goldziher/html-to-markdown).

This library started life as a fork of `markdownify`, a Python library for converting HTML to Markdown. I forked it originally because I needed modern type hints, but then found myself rewriting the entire thing. Over time it became essential for [kreuzberg](https://github.com/Goldziher/kreuzberg), where it serves as a backbone for both html -&gt; markdown and hOCR -&gt; markdown.

I am working on Kreuzberg v4, which migrates much of it to Rust. This necessitated updating this component as well, which led to a full rewrite in Rust, offering improved performance, memory stability, and a more robust feature set.

v2  delivers Rust-backed HTML ‚Üí Markdown conversion with Python bindings, a CLI and a Rust crate. The rewrite makes this by far the most performance and complete solution for HTML to Markdown conversion in python. Here are some benchmarks:

Apple M4 ‚Ä¢ Real Wikipedia documents ‚Ä¢ `convert()` (Python)

| Document            | Size  | Latency | Throughput | Docs/sec |
| ------------------- | ----- | ------- | ---------- | -------- |
| Lists (Timeline)    | 129KB | 0.62ms  | 208‚ÄØMB/s   | 1,613    |
| Tables (Countries)  | 360KB | 2.02ms  | 178‚ÄØMB/s   | 495      |
| Mixed (Python wiki) | 656KB | 4.56ms  | 144‚ÄØMB/s   | 219      |

&gt; V1 averaged ~2.5‚ÄØMB/s (Python/BeautifulSoup). V2‚Äôs Rust engine delivers 60‚Äì80x higher throughput.

The Python package still exposes `markdownify`-style calls via `html_to_markdown.v1_compat`, so migrations are relatively straightforward, although the v2 did introduce some breaking changes (see CHANGELOG.md for full details).

## Highlights

Here are the key highlights of the v2 release aside from the massive performance improvements:

- CommonMark-compliant defaults with explicit toggles when you need legacy behaviour.
- Inline image extraction (`convert_with_inline_images`) that captures data URI assets and inline SVGs with sizing and quota controls.
- Full hOCR 1.2 spec compliance, including hOCR table reconstruction and YAML frontmatter for metadata to keep OCR output structured.
- Memory is kept kept in check by dedicated harnesses: repeated conversions stay under 200‚ÄØMB RSS on multi-megabyte corpora.

## Target Audience
- Engineers replacing BeautifulSoup-based converters that fall apart on large documents or OCR outputs.
- Python, Rust, and CLI users who need identical Markdown from libraries, pipelines, and batch tools.
- Teams building document understanding stacks (including the kreuzberg ecosystem) that rely on tight memory behaviour and parallel throughput.
- OCR specialists who need to process hOCR efficiently.

## Comparison to Alternatives
- [`markdownify`](https://github.com/matthewwithanm/python-markdownify): the spiritual ancestor, but still Python + BeautifulSoup. html-to-markdown v2 keeps the API shims while delivering 60‚Äì80√ó more throughput, table-aware hOCR support, and deterministic memory usage across repeated conversions.
- [`html2text`](https://github.com/Alir3z4/html2text): solid for quick scripts, yet it lacks CommonMark compliance and tends to drift on complex tables and OCR layouts; it also allocates heavily under pressure because it was never built with long-running processes in mind.
- [`pandoc`](https://github.com/jgm/pandoc): extremely flexible (and amazing!), but large, much slower for pure HTML ‚Üí Markdown pipelines, and not embeddable in Python without subprocess juggling. html-to-markdown v2 offers a slim Rust core with direct bindings, so you keep the performance while staying in-process.

If you end up using the rewrite, a ‚≠êÔ∏è on the [repo](https://github.com/Goldziher/html-to-markdown) always makes yours truly happy!",0.9,23,0,23
Python,"I shared 300+ Python Data Science Videos on YouTube (Tutorials, Projects and Full Courses)","Hello, I am sharing free Python Data Science Tutorials for over 2 years on YouTube and I wanted to share my playlists. I believe they are great for learning the field, I am sharing them below. Thanks for reading!

Python Tutorials -&gt;¬†[https://youtube.com/playlist?list=PLTsu3dft3CWgJrlcs\_IO1eif7myukPPKJ&amp;si=fYIz2RLJV1dC6nT5](https://youtube.com/playlist?list=PLTsu3dft3CWgJrlcs_IO1eif7myukPPKJ&amp;si=fYIz2RLJV1dC6nT5)

Data Science Full Courses &amp; Projects:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWiow7L7WrCd27ohlra\_5PGH](https://youtube.com/playlist?list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH)

AI Tutorials (LangChain, LLMs &amp; OpenAI API):¬†[https://youtube.com/playlist?list=PLTsu3dft3CWhAAPowINZa5cMZ5elpfrxW](https://youtube.com/playlist?list=PLTsu3dft3CWhAAPowINZa5cMZ5elpfrxW)

Machine Learning Tutorials:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWhSJh3x5T6jqPWTTg2i6jp1](https://youtube.com/playlist?list=PLTsu3dft3CWhSJh3x5T6jqPWTTg2i6jp1)

Deep Learning Tutorials:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWghrjn4PmFZlxVBileBpMjj](https://youtube.com/playlist?list=PLTsu3dft3CWghrjn4PmFZlxVBileBpMjj)

Natural Language Processing Tutorials:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWjYPJi5RCCVAF6DxE28LoKD](https://youtube.com/playlist?list=PLTsu3dft3CWjYPJi5RCCVAF6DxE28LoKD)

Time Series Analysis Tutorials:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWibrBga4nKVEl5NELXnZ402](https://youtube.com/playlist?list=PLTsu3dft3CWibrBga4nKVEl5NELXnZ402)

Streamlit Based Python Web App Development Tutorials:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWhBViLMhL0Aqb75rkSz\_CL-](https://youtube.com/playlist?list=PLTsu3dft3CWhBViLMhL0Aqb75rkSz_CL-)

Data Cleaning Tutorials:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWhOUPyXdLw8DGy\_1l2oK1yy](https://youtube.com/playlist?list=PLTsu3dft3CWhOUPyXdLw8DGy_1l2oK1yy)

Data Analysis Tutorials:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWhwPJcaAc-k6a8vAqBx2\_0t](https://youtube.com/playlist?list=PLTsu3dft3CWhwPJcaAc-k6a8vAqBx2_0t)

End-to-End Data Science Projects:¬†[https://youtube.com/playlist?list=PLTsu3dft3CWg69zbIVUQtFSRx\_UV80OOg](https://youtube.com/playlist?list=PLTsu3dft3CWg69zbIVUQtFSRx_UV80OOg)",0.81,19,0,19
Python,Feedback Request for API Key Management Library for FastAPI,"Hello,

In my work, I build many FastAPI applications, both internal and external, that expose endpoints to other product, business, and data teams, accessible via API keys. Each project eventually ended up with its own slightly different API key system, so I finally took the time to extract the common parts and combine them into a reusable library.

[https://github.com/Athroniaeth/fastapi-api-key](https://github.com/Athroniaeth/fastapi-api-key)

Before publishing it publicly (not yet on PyPI, and the mkdocs documentation is still local), I‚Äôd like to get feedback from people who have solved similar problems (or just see what they think).

The goal is to see if I can improve this project or if there are any major security flaws (which would be problematic for an API key system).

I built the library as follows:

* **Security-first**: secrets are hashed with a salt and a pepper, and never logged or returned after creation
* **Easy-to-use**: just inherited from the repository and use service
* **Prod-ready**: services and repositories are async, and battle-tested
* **Agnostic hasher**: you can use any async-compatible hashing strategy (default: Argon2)
* **Agnostic backend**: you can use any async-compatible database (default: SQLAlchemy)
* **Factory**: create a Typer, FastAPI router wired to api key systems (only SQLAlchemy for now)

**I‚Äôd love feedback on (but not limited to) the following:**

* Are there features you would expect that don‚Äôt exist?
* Does the SQLAlchemy Mixin approach seem good for handling custom field extensions?
* Do you see any potential flaws with the current hashing/peppering strategy?
* What do you think about the extras/packaging approach (‚Äúcore‚Äù, ‚Äúfastapi‚Äù, ‚Äúall‚Äù)?

Is there anything else I should add to make it more usable? If you want to browse the code, start with the preliminary README (which includes usage examples). There‚Äôs also mkdocs documentation with quickstarts and usage guides.",1.0,16,0,16
Python,[FOSS] Flint: A 100% Config-Driven ETL Framework,"I'd like to share **Flint**, a configuration-driven ETL framework that lets you define complete data pipelines through JSON/YAML instead of code.

## What My Project Does

Flint transforms straightforward ETL workflows from programming tasks into declarative configuration. Define your sources, transformations (select, filter, join, cast, etc.), and destinations in JSON or YAML - the framework handles execution. The processing engine is abstracted away, currently supporting Apache Spark with Polars in development.

**It's not intended to replace all ETL development** - complex data engineering still needs custom code. Instead, it handles routine ETL tasks so engineers can focus on more interesting problems.

## Target Audience
- Data engineers tired of writing boilerplate for basic pipelines, so they ahve more time for more interesting programming tasks than straightforward ETL pipelines.
- Teams wanting standardized ETL patterns
- Organizations needing pipeline logic accessible to non-developers
- Projects requiring multi-engine flexibility

100% test coverage (unit + e2e), strong typing, extensive documentation with class and activity diagrams, and configurable alerts/hooks.

## Comparison

Unlike other transformation tools like DBT this one is configuration focused to reduce complexity and programming knowledge to make the boring ETL task simple, to keep more time for engineers for more intersting issues. This focuses on pure configuration without vendor lock-in as the backend key can be changed anytime with another implementation.

## Future expansion

The foundation is solid - now looking to expand with new engines, add tracing/metrics, migrate CLI to Click, move from azure devops CICD to github actions, extend Polars transformations, and more.

**[GitHub: config-driven-ETL-framework](https://github.com/krijnvanderburg/config-driven-ETL-framework)**. If you like the project idea then consider giving it a star, it means the world to get a project started from the ground.

```jsonc
{
    ""runtime"": {
        ""id"": ""customer-orders-pipeline"",
        ""description"": ""ETL pipeline for processing customer orders data"",
        ""enabled"": true,
        ""jobs"": [
            {
                ""id"": ""silver"",
                ""description"": ""Combine customer and order source data into a single dataset"",
                ""enabled"": true,
                ""engine_type"": ""spark"", // Specifies the processing engine to use
                ""extracts"": [
                    {
                        ""id"": ""extract-customers"",
                        ""extract_type"": ""file"", // Read from file system
                        ""data_format"": ""csv"", // CSV input format
                        ""location"": ""examples/join_select/customers/"", // Source directory
                        ""method"": ""batch"", // Process all files at once
                        ""options"": {
                            ""delimiter"": "","", // CSV delimiter character
                            ""header"": true, // First row contains column names
                            ""inferSchema"": false // Use provided schema instead of inferring
                        },
                        ""schema"": ""examples/join_select/customers_schema.json"" // Path to schema definition
                    }
                ],
                ""transforms"": [
                    {
                        ""id"": ""transform-join-orders"",
                        ""upstream_id"": ""extract-customers"", // First input dataset from extract stage
                        ""options"": {},
                        ""functions"": [
                            {""function_type"": ""join"", ""arguments"": {""other_upstream_id"": ""extract-orders"", ""on"": [""customer_id""], ""how"": ""inner""}},
                            {""function_type"": ""select"", ""arguments"": {""columns"": [""name"", ""email"", ""signup_date"", ""order_id"", ""order_date"", ""amount""]}}
                        ]
                    }
                ],
                ""loads"": [
                    {
                        ""id"": ""load-customer-orders"",
                        ""upstream_id"": ""transform-join-orders"", // Input dataset for this load
                        ""load_type"": ""file"", // Write to file system
                        ""data_format"": ""csv"", // Output as CSV
                        ""location"": ""examples/join_select/output"", // Output directory
                        ""method"": ""batch"", // Write all data at once
                        ""mode"": ""overwrite"", // Replace existing files if any
                        ""options"": {
                            ""header"": true // Include header row with column names
                        },
                        ""schema_export"": """" // No schema export
                    }
                ],
                ""hooks"": {
                    ""onStart"": [], // Actions to execute before pipeline starts
                    ""onFailure"": [], // Actions to execute if pipeline fails
                    ""onSuccess"": [],  // Actions to execute if pipeline succeeds
                    ""onFinally"": [] // Actions to execute after pipeline completes (success or failure)
                }
            }
        ]
    }
}
```",0.73,5,0,5
Python,sdax - an API for asyncio for handling parallel tasks declaratively,"Parallel async¬†is fast, but managing failures and cleanup across¬†multiple dependent operations is hard.

sdax - (Structured Declarative Async eXecution) does all the heavy lifting. You just need to write the async functions and wire them into ""levels"".

I'm working on an extension to sdax for doing all the initialization using decorators - coming next.

Requires Python 3.11 or higher since it uses asyncio.TaskGroup and ExceptionGroup which were introduced in 3.11.

See: [https://pypi.org/project/sdax](https://pypi.org/project/sdax/), [https://github.com/owebeeone/sdax](https://github.com/owebeeone/sdax)",0.7,4,0,4
Python,uv cheatsheet with most common/useful commands,"I've been having lots of fun using Astral's uv and also teaching it to friends and students, so I decided to create a cheatsheet with the most common/useful commands.

[uv cheatsheet with most common/useful commands](https://mathspp.com/blog/uv-cheatsheet)

I included sections about

 - project creation;
 - dependency management;
 - project lifecycle &amp; versioning;
 - installing/working with tools;
 - working with scripts;
 - uv's interface for `pip` and `venv`; and
 - some meta &amp; miscellaneous commands.

The link above takes you to a page with all these sections as regular tables and to high-resolution/print-quality downloadable files you can  get for yourself from the link above.

I hope this is helpful for you and if you have any feedback, I'm all ears!",0.97,324,0,324
Python,How much Python do I really need to know to land my first dev job?,"Hey everyone,
I‚Äôve been working as a Data Analyst at an energy distribution company for about a year and a half. My long-term goal has always been to build the skills needed to transition into a developer role.
I feel like it‚Äôs finally time to sharpen my knowledge and make that pivot ‚Äî but honestly, I still feel like I know nothing, even though I‚Äôm a bit of a Swiss Army knife in my current job.
Here‚Äôs a quick overview of what I already know and where I‚Äôm at:
Several Python certificates (Coursera and Cisco).
Certified and experienced in SQL databases (DDL and DML).
Comfortable working with Linux systems.
Process automation experience using PDI Spoon and batch scripts.
Currently studying Data Analytics and Machine Learning with Python.
I haven‚Äôt worked with APIs or HTTP requests yet, and my English level is low, but I‚Äôm improving.
Where should I focus next? Do I need to go deeper in Python itself, or start learning web frameworks, APIs, or something else to move toward a dev job?",0.61,18,0,18
Python,Built an automated GitHub-RAG pipeline system with incremental sync,"**What My Project Does**

RAGIT is a fully automated RAG pipeline for GitHub repositories. Upload a repo and it handles collection, preprocessing, embedding, vector indexing, and incremental synchronization automatically. Context is locked to specific commits to avoid version confusion. When you ask questions, hybrid search finds relevant code with citations and answers consistently across multiple files.

**Target Audience**

Production-ready system for development teams working with large codebases. Built with microservices architecture (Gateway-Backend-Worker pattern) using PostgreSQL, Redis, and Milvus. Fully dockerized for easy deployment. Useful for legacy code analysis, project onboarding, and ongoing codebase understanding.

**Comparison**

Unlike manually copying code into ChatGPT/Claude which loses context and version tracking, RAGIT automates the entire pipeline and maintains commit-level consistency. Compared to other RAG frameworks that require manual chunking and indexing, RAGIT handles GitHub repos end-to-end with automatic sync when code changes. More reproducible and consistent than direct LLM usage.

Apache 2.0 licensed.

GitHub: https://github.com/Gyu-Chul/RAGIT
Demo: https://www.youtube.com/watch?v=VSBDDvj5_w4

Open to feedback.",0.43,0,0,0
Python,UV on termux Debian (android),"Anybody managed to build it? And if so, pretty please with chocolate chips, how? I've made the obvious attempts (pip install, cargo...) but no joy so far.",0.75,2,0,2
Python,PipeFunc: Build Lightning-Fast Pipelines with Python: DAGs Made Easy,"Hey r/Python!

I'm excited to share `pipefunc` ([github.com/pipefunc/pipefunc](https://github.com/pipefunc/pipefunc)), a Python library designed to make building and running complex computational workflows incredibly fast and easy. If you've ever dealt with intricate dependencies between functions, struggled with parallelization, or wished for a simpler way to create and manage DAG pipelines, `pipefunc` is here to help.

**What My Project Does:**

`pipefunc` empowers you to easily construct **Directed Acyclic Graph (DAG)** pipelines in Python. It handles:

1. **Automatic Dependency Resolution:**  `pipefunc` automatically determines the correct execution order of your functions, eliminating manual dependency management.
2. **Lightning-Fast Execution:** With minimal overhead (around **10 ¬µs per function call**), `pipefunc` ensures your pipelines run super fast.
3. **Effortless Parallelization:** `pipefunc` automatically parallelizes independent tasks, whether on your local machine or a SLURM cluster. It supports any `concurrent.futures.Executor`!
4. **Intuitive Visualization:** Generate interactive graphs to visualize your pipeline's structure and understand data flow.
5. **Simplified Parameter Sweeps:**  `pipefunc`'s `mapspec` feature lets you easily define and run N-dimensional parameter sweeps, which is perfect for scientific computing, simulations, and hyperparameter tuning.
6. **Resource Profiling:** Gain insights into your pipeline's performance with detailed CPU, memory, and timing reports.
7. **Caching:** Avoid redundant computations with multiple caching backends.
8. **Type Annotation Validation:** Ensures type consistency across your pipeline to catch errors early.
9. **Error Handling:** Includes an `ErrorSnapshot` feature to capture detailed information about errors, making debugging easier.

**Target Audience:**

`pipefunc` is ideal for:

*   **Scientific Computing:** Streamline simulations, data analysis, and complex computational workflows.
*   **Machine Learning:** Build robust and reproducible ML pipelines, including data preprocessing, model training, and evaluation.
*   **Data Engineering:** Create efficient ETL processes with automatic dependency management and parallel execution.
*   **HPC:** Run `pipefunc` on a SLURM cluster with minimal changes to your code.
*   **Anyone** working with interconnected functions who wants to improve code organization, performance, and maintainability.

`pipefunc` is designed to be flexible (great tool for prototyping and experimentation) and easy to adopt!

**Comparison:**

*  **vs. Hamilton:** Hamilton also compiles Python functions into
DAGs, but it centers on column-level DataFrame engineering, ships modifiers like @with_columns/@extract_columns, and offers built-in data/schema validation plus an optional UI for lineage and observability; pipefunc leans toward low-overhead scientific/HPC pipelines, executor-agnostic parallelism, and N-D sweeps via mapspecs.
*   **vs. Dask:** `pipefunc` offers a higher-level, more declarative way to define pipelines. It automatically manages task scheduling and execution based on your function definitions and `mapspec`s, without requiring you to write explicit parallel code.
*   **vs. Luigi/Airflow/Prefect/Kedro:** While those tools excel at ETL and event-driven workflows, `pipefunc` focuses on scientific computing, simulations, and computational workflows where fine-grained control over execution and resource allocation is crucial. Also, it's way easier to setup and develop with, with minimal dependencies!
*   **vs. Pandas:** You can easily combine `pipefunc` with `Pandas`! Use `pipefunc` to manage the execution of `Pandas` operations and parallelize your data processing pipelines. But it also works well with `Polars`, `Xarray`, and other libraries!
*   **vs. Joblib:** `pipefunc` offers several advantages over `Joblib`. `pipefunc` automatically determines the execution order of your functions, generates interactive visualizations of your pipeline, profiles resource usage, and supports multiple caching backends. Also, `pipefunc` allows you to specify the mapping between inputs and outputs using `mapspec`s, which enables complex map-reduce operations.

**Examples:**

**Simple Example:**

```python
from pipefunc import pipefunc, Pipeline

@pipefunc(output_name=""c"")
def add(a, b):
    return a + b

@pipefunc(output_name=""d"")
def multiply(b, c):
    return b * c

pipeline = Pipeline([add, multiply])
result = pipeline(""d"", a=2, b=3)  # Automatically executes 'add' first
print(result)  # Output: 15

pipeline.visualize() # Visualize the pipeline
```

**Parallel Example with `mapspec`:**

Parallelizes for all combinations of inputs `a` and `b` automatically!

```python
import numpy as np
from pipefunc import pipefunc, Pipeline
from pipefunc.map import load_outputs

@pipefunc(output_name=""c"", mapspec=""a[i], b[j] -&gt; c[i, j]"")
def f(a: int, b: int):
    return a + b

@pipefunc(output_name=""mean"") # no mapspec, so receives 2D `c[:, :]`
def g(c: np.ndarray):
    return np.mean(c)

pipeline = Pipeline([f, g])
inputs = {""a"": [1, 2, 3], ""b"": [4, 5, 6]}
result_dict = pipeline.map(inputs, run_folder=""my_run_folder"", parallel=True)
result = load_outputs(""mean"", run_folder=""my_run_folder"") # can load now too
print(result)  # Output: 7.0
```

**Getting Started:**

*   **Docs:** [https://pipefunc.readthedocs.io/](https://pipefunc.readthedocs.io/)
*   **Source:** [https://github.com/pipefunc/pipefunc](https://github.com/pipefunc/pipefunc)

I'm exctited to hear your feedback and answer any questions you have. Give `pipefunc` a try and let me know how it can improve your workflows!
",0.95,42,0,42
Python,SPDL - Scalable and Performant Data Loading,"Hi Python community,

Inspired by recent showcases on pipeline libraries ([Pipevine](https://www.reddit.com/r/Python/comments/1o2n119/ergonomic_concurrency/), [pipefunc](https://www.reddit.com/r/Python/comments/1o3323m/pipefunc_build_lightningfast_pipelines_with/)), I‚Äôd like to share my project:¬†**SPDL (Scalable and Performant Data Loading)**. 

# What My Project Does

SPDL is designed to address the data loading bottleneck in machine learning (ML) and AI training pipelines. You break down data loading into discrete tasks with different constraints (network, CPU, GPU transfer etc) and construct a pipeline, and SPDL executes them efficiently. It features a task execution engine (pipeline abstraction) built on asyncio, alongside an independent I/O module for media processing. 

# Resources:

* **Repo:**¬†[https://github.com/facebookresearch/spdl](https://github.com/facebookresearch/spdl)
* **Documentation:**¬†[SPDL Docs](https://facebookresearch.github.io/spdl/main/)
* **PyPI:**
   * Install with `pip install spdl`
      * [spdl-core](https://pypi.org/project/spdl-core/) (no dependency)
      * [spdl-io](https://pypi.org/project/spdl-io/) (requires NumPy, and optionally PyTorch / Numba / Jax) 
* **arXiv:** [2504.20067](https://arxiv.org/abs/2504.20067)

# Target Audience

ML practitioners whose focus is model training rather than software engineering. It is production-ready.

# Core Principles

* **High Throughput &amp; Efficiency:**¬†SPDL maximizes data loading speed and minimizes CPU/memory overhead to keep GPUs busy.
* **Flexibility:**¬†The pipeline abstraction is highly customizable, allowing users to tailor the structure to their environment, data, and requirements.
* **Observability:**¬†SPDL provides runtime statistics for each pipeline component, helping users identify bottlenecks and optimize performance.
* **Intuitive Construction:**¬†Pipelines are easy to build and reason about, with clear separation of stages and bounding factors.

# Architecture Overview

* **Pipeline Abstraction:**¬†With SPDL, you break down data loading into discrete tasks with different constraints (network, CPU, GPU transfer etc) and construct a pipeline that executes each task concurrently.
* **Multi-threading &amp; Multi-processing:**¬†SPDL uses multi-threading by default for parallelism, with optional multi-processing for workloads that benefit from process isolation. In production, we‚Äôve successfully used multi-threading with Python 3.10 by composing functions that release the GIL. Support for InterpreterPoolExecutor in Python 3.14 is planned.
* **Async Event Loop:**¬†The task execution engine is built on an async event loop, supporting both async and regular functions.
* **Media I/O Module:**¬†Includes a high-performance I/O module for audio, video, and image processing, designed from scratch for maximum throughput. It also supports loading NumPy array fast from memory.
* **Non-invasive:** SPDL orchestrates the execution of given functions, and the only requirement for the function is that it is univariate function. No requirements to change your algorithms/business logic to pipelining it with SPDL.

# Monitoring &amp; Optimization

SPDL exports detailed runtime statistics for each pipeline stage, making it easy to monitor throughput, resource usage, and identify bottlenecks. For more on production bottleneck analysis, see the¬†[Optimization Guide](https://l.facebook.com/l.php?u=https%3A%2F%2Ffacebookresearch.github.io%2Fspdl%2Fmain%2Foptimization_guide%2Fanalysis.html&amp;h=AT3Hi60CVKm_FzR6YfXpfbCm4kDWAVz5144UjUlM7PbqiIaM-7yzNqESVRJqwnHieAIYd_BoPn5GkG8-t66ZfmbEbyS3hyMsNXu-CqwngZ5FVw1ZdbXwhRVG1xYKrE4d81n7m4rYPBbi2XQBwy898DT9nWE).

# Comparison

* Unlike previously shared projects, the feature set is more specific to ML efficiency. (though the pipeline abstraction is generic, and library is agnostic to ML framework)
* Supports single chain pipelining with different concurrency. Merging pipeline is also supported but not branching or general graph structure.",0.77,11,0,11
Python,EPUBLib - New python library for creating and editing EPUB3 files,"I wrote a python library to edit and create EPUB3 files.

* Repository:¬†[gitlab.com/joaoseckler/epublib](http://gitlab.com/joaoseckler/epublib)
* PyPi:¬†[pypi.org/project/epublib/](http://pypi.org/project/epublib/)
* Documentation:¬†[epub-lib.readthedocs.io/en/latest/](http://epub-lib.readthedocs.io/en/latest/)

Any suggestions and criticisms are welcome! And if you know any other places where people might be interested in this tool, please let me know.

**What My Project Does:**

It is a library for creating and editing EPUB documents according to the EPUB3 specification. Example from the [documentation](https://epub-lib.readthedocs.io/en/latest/index.html#basic-usage):

    from epublib import EPUB
    
    with EPUB(""book.epub"") as book:
        book.metadata.title = ""New title""
    
        for doc in book.documents:
            new_script = doc.soup.new_tag(""script"", attrs={""src"": ""../Misc/myscript.js""})
            doc.soup.head.append(new_script)
    
            new_heading = doc.soup.new_tag(""h1"", string=""New heading"")
            doc.soup.body.insert(0, new_heading)
    
        book.update_manifest_properties()
        book.write(""book-modified.epub"")

See the¬†[usage section¬†](https://epub-lib.readthedocs.io/en/latest/#basic-usage)of the documentation for a more usage examples.

**Target Audience:**

People working with publishing digital books using the EPUB format.

**Comparison:**

There is already an active python library called¬†[EbookLib](https://pypi.org/project/EbookLib/)¬†for handling EPUBs. A few things EPUBLib does differently:

1. Handles the EPUB non-intrusively, e.g. won't regenerate the package document/metadata before writing, can edit toc without recreating the entire navigation document;
2. Built-in XML parsing with BeautifulSoup;
3. Extra features: rename files, remove files, spine reordering etc;
4. Use nomenclature from the specification when possible (e.g. ""resource"" instead of ""item"").

",0.87,15,0,15
Python,Sell me (and my team) on UV,"I think UV is great so far, I only recently started using it. I would like to move myself and my team to using it as our official package manager, but I don‚Äôt really know the extent of why ‚Äúthis tool is better than venv/pip‚Äù. It was hard enough to convince them we should be using venv in the first place, but now I feel like I‚Äôm trying to introduce a tool that adds seemingly quite a bit more complexity.

Just curious on all the benefits and what I can say to encourage the movement.

Thanks!",0.35,0,0,0
Python,Vision Agents 0.1,"First steps here, we've just released 0.1 of Vision Agents. [https://github.com/GetStream/Vision-Agents](https://github.com/GetStream/Vision-Agents)

**What My Project Does**

The idea is that it makes it super simple to build vision agents, combining fast models like Yolo with Gemini/Openai realtime. We're going for low latency &amp; a completely open sdk. So you can use any vision model or video edge network. 

Here's an example of running live video through Yolo and then passing it to Gemini

    agent = Agent(
        edge=getstream.Edge(),
        agent_user=agent_user,
        instructions=""Read @golf_coach.md"",
        llm=openai.Realtime(fps=10),
        #llm=gemini.Realtime(fps=1), # Careful with FPS can get expensive
        processors=[ultralytics.YOLOPoseProcessor(model_path=""yolo11n-pose.pt"")],
    )

**Target Audience**¬†

Vision AI is like chatgpt in 2022. It's really fun to see how it works and what's possible. Anything from live coaching, to sports, to physical therapy, robotics, drones etc. But it's not production quality yet. Gemini and OpenAI both hallucinate a ton for vision AI. It seems close to being viable though, especially fun to have it describe your surroundings etc.

**Comparison**

Similar to Livekit Agents (livekit specific) and Pipecat (daily). We're going for open to all edge networks, low latency and with a focus on vision AI (voice works, but we're focused on live video)

This has been fun to work on with the team, finally at 0.1 :)



",0.8,17,0,17
Python,Intermediate-level project suggestions,"I need intermediate-level project ideas that I can do with Python. Other languages can be added to the project as well, that‚Äôs not a problem. They need to look good on GitHub and on my CV.",0.18,0,0,0
Python,Fiatlight: Instantly turn Python functions into interactive GUI apps or workflows,"**What Fiatlight Does**

[Fiatlight](https://pthom.github.io/fiatlight_doc) is a Python toolkit that lets you build interactive graphical applications by providing an *automatic user interface* for functions and dataclasses. It is published under the MIT license.

You may think of Fiatlight as *""ComfyUI for any type of data and functions""*: easy visual and interactive pipelines for any domain of interest. You do not have to write any UI code, and you can connect multiple functions to build workflows instantly: the outputs flow from one function to the next

Users can then adjust every parameter of the functions and save/reload their work. 

Fiatlight is built on top of Dear ImGui Bundle. It is very fast, and can provide feedback in real-time (at 120 FPS!). Since Dear ImGui Bundle is available via Pyodide, Fiatlight applications can be used locally or deployed as static web pages, without any server-side component.

As a prototyping tool, fiatlight does *not* provide full design control over the UI. It does however provide advanced viewer for many data types (standard python types, images, files, dataframes, matplotlib figures,  etc.), and is easily extensible.
 
*Links:*
- [Video tutorials](https://pthom.github.io/fiatlight_doc/flgt/video_tutorials.html)
- [Documentation](https://pthom.github.io/fiatlight_doc)
- [GitHub repo](https://github.com/pthom/fiatlight)


**Target Audience**

* Hobbyists wanting to create interactive applications quickly
* Educators and instructors needing interactive tools for teaching programming or algorithms
* Researchers who need shareable demos or visualizations of their work
* Developers who want to fine tune their algorithms, with visual feedback
* Library authors who want to showcase or demonstrate how to use and compose their functions
* Data scientists and analysts wanting instant GUI dashboards for exploring data


**Comparison**

* Broader scope than ComfyUI
* Often faster than streamlit or gradio (runs locally or serverless on a static web page)
* LabVIEW is famous for data acquisition, hardware integration, and quick GUI building, but is expensive and highly niche/proprietary
* Unreal Blueprints are widely used for visual scripting in games and rapid prototyping, but tightly coupled to Unreal Engine and less suitable for general Python/data workflows

**Example**

The example below showcases a simple pipeline where the user edits the input float value, and automatically sees the output of each function. Widgets for each parameter are generated according to type and customized with attributes.

```python
# Our functions
def float_source(x: float = 1.0) -&gt; float:
    return x
def double(x: float = 1.0) -&gt; float:
    return 2 * x

# Below, our GUI, where the user can edit the input for float_source, and see the output of both functions

import fiatlight as fl
# Set range for slider
fl.add_fiat_attributes(float_source, x__edit_type=""slider"", x__range=(0.0, 10.0))  

# Display a GUI for the composition of these two functions
fl.run([float_source, double])  
```
",0.77,23,0,23
Python,Neend some career advice,I am bpharm 4 yr student and I wanted to work in the field of programming and development I basically have no knowledge about programming skills I am currently 22 yr should I pursue with programming or I should just stick to the pharmacy ,0.31,0,0,0
Python,Looking for a Python developer to port a stable Perl CGI project,"Hello everyone,

I‚Äôve built a fully functional Perl CGI platform that‚Äôs stable and bug-free. It‚Äôs a small but complete project that has been running reliably.

At this point, I‚Äôd love to see a developer take it over and help port it to Python (Flask or FastAPI). The goal is to make it more accessible and maintainable for the broader community.

GitHub repo: https://github.com/socaltiger/BatchSubmit.com

Why this could be interesting for you:
- The project is already complete and working ‚Äì you won‚Äôt have to start from scratch.
- A chance to practice porting from Perl to Python on a real, running platform.
- You‚Äôll get to modernize a project and contribute to open source in a meaningful way.

If you‚Äôre interested, feel free to fork, submit PRs, or reach out via GitHub issues.  
I‚Äôm happy to guide the process and provide background on the project‚Äôs design.

Looking forward to seeing some talent breathe new life into it!",0.3,0,0,0
Python,Ergonomic Concurrency,"**Project name:** Pipevine  
**Project link:** [https://github.com/arrno/pipevine](https://github.com/arrno/pipevine)

**What My Project Does**  
Pipevine is a lightweight async pipeline and worker-pool library for Python.  
It helps you compose concurrent dataflows with backpressure, retries, and cancellation.. without all the asyncio boilerplate.

**Target Audience**  
Developers who work with data pipelines, streaming, or CPU/IO-bound workloads in Python.  
It‚Äôs designed to be production-ready but lightweight enough for side projects and experimentation.

**How to Get Started**

    pip install pipevine

    import asyncio
    from pipevine import Pipeline, work_pool
    
    @work_pool(buffer=10, retries=3, num_workers=4)
    async def process_data(item, state):
        # Your processing logic here
        return item * 2
    
    @work_pool(buffer=5, retries=1)
    async def validate_data(item, state):
        if item &lt; 0:
            raise ValueError(""Negative values not allowed"")
        return item
    
    # Create and run pipeline
    pipe = Pipeline(range(100)) &gt;&gt; process_data &gt;&gt; validate_data
    result = await pipe.run()
    

**Feedback Requested**  
I‚Äôd love thoughts on:

* API ergonomics (does it feel Pythonic?)
* Use cases where this could simplify your concurrency setup
* Naming and documentation clarity",0.8,22,0,22
Python,Automating the Upgrade to Python 3.14,"I detailed the process I followed to get OpenAI‚Äôs codex cli  to upgrade a complex project with lots of dependencies to python 3.14 with uv:

https://x.com/doodlestein/status/1976478297744699771?s=46

Charlie Marsh retweeted it, so you can trust that it‚Äôs not a bunch of nonsense! Hope you guys find it useful.",0.33,0,0,0
